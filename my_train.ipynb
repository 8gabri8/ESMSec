{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a1f68d",
   "metadata": {},
   "source": [
    "# Necessary\n",
    "\n",
    "- Create VE\n",
    "- Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09bf533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gdallagl/.pyenv/versions/esm2_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/home/gdallagl/myworkdir/ESMSec/utils\"))  # Adds current folder to Python path\n",
    "\n",
    "import utils.my_functions as mf\n",
    "import utils.models as my_models\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration / hyperparameters\n",
    "config = {\n",
    "    \"FLUID\": \"CSF\",         # Dataset name\n",
    "    \"NUM_ITERS\": 500,        # Total iterations\n",
    "    \"BATCH_SIZE\": 32,       # Batch size\n",
    "    \"LR\": 5e-5,             # Learning rate\n",
    "    \"LR_DECAY_GAMMA\": 1,    # Learning rate decay\n",
    "    \"LR_DECAY_STEPS\": 1000, # Learning rate decay steps\n",
    "    \"EVAL_SIZE\": 100,       # Evaluation frequency\n",
    "    \"SEED\": 43215,           # Random seed\n",
    "    \"PROTEIN_MAX_LENGTH\": 1000, # Max protein length (for ESM2)\n",
    "    \"PRETRAIN_ESM_CHECKPOINT_NAME\": \"facebook/esm2_t6_8M_UR50D\", # ESM2 model name\n",
    "    \"PRETRAIN_ESM_CACHE_DIR\": \"/home/gdallagl/myworkdir/data/esm2-models\", # ESM2 model cache dir\n",
    "    \"DATASET_PATH\": \"/home/gdallagl/myworkdir/data/ESMSec/protein/CSF_my_dataset.csv\", # Path to dataset\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\", # Device to use (cuda or cpu)\n",
    "    \"PATH_TO_SAVE_MODEL\": \"/home/gdallagl/myworkdir/models/ESMSec/CSF_trained_model.pth\" # Path to save the model\n",
    "}\n",
    "\n",
    "# Initializations\n",
    "random.seed(config[\"SEED\"])\n",
    "np.random.seed(config[\"SEED\"])\n",
    "torch.manual_seed(config[\"SEED\"])\n",
    "torch.backends.cudnn.benchmark = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabfbe3b",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d1f2e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ESM model type <class 'transformers.models.esm.modeling_esm.EsmModel'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ESM model\n",
    "esm_model = AutoModel.from_pretrained(config[\"PRETRAIN_ESM_CHECKPOINT_NAME\"],  cache_dir=config[\"PRETRAIN_ESM_CACHE_DIR\"])\n",
    "# Checj whcih model has been moded by AutoModel.from_pretrained()\n",
    "print(\"\\nESM model type\", type(esm_model), \"\\n\")\n",
    "\n",
    "# Load relative tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"PRETRAIN_ESM_CHECKPOINT_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27d141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ESM hidden dim 320 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialise model (ESM+ HEad)\n",
    "net = my_models.EsmDeepSec(esm_model).to(config[\"DEVICE\"])\n",
    "# hidden dim of final embeggin of each aa after trnafoerm \n",
    "print(\"\\nESM hidden dim\", net.ESM_hidden_dim, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a999eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block ESM paramters to be trained\n",
    "for param in net.esm_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b20e5",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b5b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "      <th>trunc_sequence</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P22694</td>\n",
       "      <td>MGNAATAKKGSEVESVKEFLAKAKEDFLKKWENPTQNNAGLEDFER...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>MGNAATAKKGSEVESVKEFLAKAKEDFLKKWENPTQNNAGLEDFER...</td>\n",
       "      <td>[tensor(0), tensor(20), tensor(6), tensor(17),...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q8NEV1</td>\n",
       "      <td>MSGPVPSRARVYTDVNTHRPREYWDYESHVVEWGNQDDYQLVRKLG...</td>\n",
       "      <td>1</td>\n",
       "      <td>validation</td>\n",
       "      <td>MSGPVPSRARVYTDVNTHRPREYWDYESHVVEWGNQDDYQLVRKLG...</td>\n",
       "      <td>[tensor(0), tensor(20), tensor(8), tensor(6), ...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q6P3V2</td>\n",
       "      <td>MPANWTSPQKSSALAPEDHGSSYEGSVSFRDVAIDFSREEWRHLDP...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>MPANWTSPQKSSALAPEDHGSSYEGSVSFRDVAIDFSREEWRHLDP...</td>\n",
       "      <td>[tensor(0), tensor(20), tensor(14), tensor(5),...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q8N0Y7</td>\n",
       "      <td>MAAYKLVLIRHGESTWNLENRFSCWYDADLSPAGHEEAKRGGQALR...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>MAAYKLVLIRHGESTWNLENRFSCWYDADLSPAGHEEAKRGGQALR...</td>\n",
       "      <td>[tensor(0), tensor(20), tensor(5), tensor(5), ...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q9NY65</td>\n",
       "      <td>MRECISVHVGQAGVQIGNACWELFCLEHGIQADGTFDAQASKINDD...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>MRECISVHVGQAGVQIGNACWELFCLEHGIQADGTFDAQASKINDD...</td>\n",
       "      <td>[tensor(0), tensor(20), tensor(10), tensor(9),...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  protein                                           sequence  label  \\\n",
       "0  P22694  MGNAATAKKGSEVESVKEFLAKAKEDFLKKWENPTQNNAGLEDFER...      1   \n",
       "1  Q8NEV1  MSGPVPSRARVYTDVNTHRPREYWDYESHVVEWGNQDDYQLVRKLG...      1   \n",
       "2  Q6P3V2  MPANWTSPQKSSALAPEDHGSSYEGSVSFRDVAIDFSREEWRHLDP...      1   \n",
       "3  Q8N0Y7  MAAYKLVLIRHGESTWNLENRFSCWYDADLSPAGHEEAKRGGQALR...      1   \n",
       "4  Q9NY65  MRECISVHVGQAGVQIGNACWELFCLEHGIQADGTFDAQASKINDD...      1   \n",
       "\n",
       "          set                                     trunc_sequence  \\\n",
       "0       train  MGNAATAKKGSEVESVKEFLAKAKEDFLKKWENPTQNNAGLEDFER...   \n",
       "1  validation  MSGPVPSRARVYTDVNTHRPREYWDYESHVVEWGNQDDYQLVRKLG...   \n",
       "2       train  MPANWTSPQKSSALAPEDHGSSYEGSVSFRDVAIDFSREEWRHLDP...   \n",
       "3       train  MAAYKLVLIRHGESTWNLENRFSCWYDADLSPAGHEEAKRGGQALR...   \n",
       "4       train  MRECISVHVGQAGVQIGNACWELFCLEHGIQADGTFDAQASKINDD...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [tensor(0), tensor(20), tensor(6), tensor(17),...   \n",
       "1  [tensor(0), tensor(20), tensor(8), tensor(6), ...   \n",
       "2  [tensor(0), tensor(20), tensor(14), tensor(5),...   \n",
       "3  [tensor(0), tensor(20), tensor(5), tensor(5), ...   \n",
       "4  [tensor(0), tensor(20), tensor(10), tensor(9),...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 20,  6, 17,  5,  5, 11,  5, 15, 15,  6,  8,  9,  7,  9,  8,  7, 15,\n",
      "         9, 18,  4,  5, 15,  5, 15,  9, 13, 18,  4, 15, 15, 22,  9, 17, 14, 11,\n",
      "        16, 17, 17,  5,  6,  4,  9, 13, 18,  9, 10, 15, 15, 11,  4,  6, 11,  6,\n",
      "         8, 18,  6, 10,  7, 20,  4,  7, 15, 21, 15,  5, 11,  9, 16, 19, 19,  5,\n",
      "        20, 15, 12,  4, 13, 15, 16, 15,  7,  7, 15,  4, 15, 16, 12,  9, 21, 11,\n",
      "         4, 17,  9, 15, 10, 12,  4, 16,  5,  7, 17, 18, 14, 18,  4,  7, 10,  4,\n",
      "         9, 19,  5, 18, 15, 13, 17,  8, 17,  4, 19, 20,  7, 20,  9, 19,  7, 14,\n",
      "         6,  6,  9, 20, 18,  8, 21,  4, 10, 10, 12,  6, 10, 18,  8,  9, 14, 21,\n",
      "         5, 10, 18, 19,  5,  5, 16, 12,  7,  4, 11, 18,  9, 19,  4, 21,  8,  4,\n",
      "        13,  4, 12, 19, 10, 13,  4, 15, 14,  9, 17,  4,  4, 12, 13, 21, 16,  6,\n",
      "        19, 12, 16,  7, 11, 13, 18,  6, 18,  5, 15, 10,  7, 15,  6, 10, 11, 22,\n",
      "        11,  4, 23,  6, 11, 14,  9, 19,  4,  5, 14,  9, 12, 12,  4,  8, 15,  6,\n",
      "        19, 17, 15,  5,  7, 13, 22, 22,  5,  4,  6,  7,  4, 12, 19,  9, 20,  5,\n",
      "         5,  6, 19, 14, 14, 18, 18,  5, 13, 16, 14, 12, 16, 12, 19,  9, 15, 12,\n",
      "         7,  8,  6, 15,  7, 10, 18, 14,  8, 21, 18,  8,  8, 13,  4, 15, 13,  4,\n",
      "         4, 10, 17,  4,  4, 16,  7, 13,  4, 11, 15, 10, 18,  6, 17,  4, 15, 17,\n",
      "         6,  7,  8, 13, 12, 15, 11, 21, 15, 22, 18,  5, 11, 11, 13, 22, 12,  5,\n",
      "        12, 19, 16, 10, 15,  7,  9,  5, 14, 18, 12, 14, 15, 18, 10,  6,  8,  6,\n",
      "        13, 11,  8, 17, 18, 13, 13, 19,  9,  9,  9, 13, 12, 10,  7,  8, 12, 11,\n",
      "         9, 15, 23,  5, 15,  9, 18,  6,  9, 18,  2,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(config[\"DATASET_PATH\"])\n",
    "\n",
    "# Truncate sequences and add as a new column\n",
    "data['trunc_sequence'] = data['sequence'].apply(\n",
    "    lambda seq: mf.truncate_sequence(seq, max_length=config[\"PROTEIN_MAX_LENGTH\"])\n",
    ")\n",
    "\n",
    "# Tokenize all sequences at once (vectorized)\n",
    "tokenized = tokenizer(\n",
    "    data['trunc_sequence'].tolist(),\n",
    "    padding='max_length',\n",
    "    max_length=config[\"PROTEIN_MAX_LENGTH\"],\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Add tokenized input_ids and attention_mask to DataFrame\n",
    "data['input_ids'] = list(tokenized['input_ids'])\n",
    "data['attention_mask'] = list(tokenized['attention_mask'])\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels_tensor = torch.tensor(data['label'].values)\n",
    "\n",
    "# Split indices\n",
    "train_idx = data['set'] == 'train'\n",
    "valid_idx = data['set'] == 'validation'\n",
    "test_idx  = data['set'] == 'test'\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(\n",
    "    tokenized['input_ids'][train_idx],\n",
    "    tokenized['attention_mask'][train_idx],\n",
    "    labels_tensor[train_idx]\n",
    ")\n",
    "\n",
    "valid_dataset = TensorDataset(\n",
    "    tokenized['input_ids'][valid_idx],\n",
    "    tokenized['attention_mask'][valid_idx],\n",
    "    labels_tensor[valid_idx]\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    tokenized['input_ids'][test_idx],\n",
    "    tokenized['attention_mask'][test_idx],\n",
    "    labels_tensor[test_idx]\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dl = DataLoader(train_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=True, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False, pin_memory=True)\n",
    "test_dl  = DataLoader(test_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False, pin_memory=True)\n",
    "\n",
    "# Optional: inspect the DataFrame\n",
    "display(data.head(5))\n",
    "print(data.loc[0, \"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d026aad",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mf.train(net, train_dl, valid_dl, test_dl, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1975c9",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e765b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.summarize_training(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc936e",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, config[\"PATH_TO_SAVE_MODEL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddce270",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f9884",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(config[\"PATH_TO_SAVE_MODEL\"])\n",
    "model.to(config[\"DEVICE\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f340a",
   "metadata": {},
   "source": [
    "### Umaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb2d2c",
   "metadata": {},
   "source": [
    "### A-scanning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
