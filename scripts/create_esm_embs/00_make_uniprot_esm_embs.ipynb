{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb7dcbc",
   "metadata": {},
   "source": [
    "# Make ESM embs of entire reviewed Unirpot\n",
    "\n",
    "- HOW TO TRUNCATE\n",
    "- WHICH MODEL\n",
    "- WHICH EMB\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload extension\n",
    "%load_ext autoreload\n",
    "# Automatically reload all modules (except built-ins) before executing code\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.chdir(\"/home/gdallagl/myworkdir/ESMSec\")\n",
    "print(f\"Working directory successfully changed to: {os.getcwd()}\")\n",
    "\n",
    "import utils.my_functions as mf\n",
    "import utils.embeddings_functions as my_embs\n",
    "import utils.dataset as my_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bce7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration / hyperparameters\n",
    "config = {\n",
    "    \"SEED\": 42,             # Random seed\n",
    "\n",
    "    \"PROTEIN_MAX_LENGTH\": 1000, # Max protein length (for ESM2)\n",
    "    \"PRETRAIN_ESM_CACHE_DIR\": \"/home/gdallagl/myworkdir/data/esm2-models\", # ESM2 model cache dir\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\", # Device to use (cuda or cpu)\n",
    "    \"BATCH_SIZE\": 16,\n",
    "\n",
    "    \"TYPE_EMB_FOR_CLASSIFICATION\": \"agg_mean\", #\"concat(agg_mean, agg_max)\", #\"contextualized_embs\", #\"concat(agg_mean, agg_max)\",cls, stacked_linearized_all, agg_mean\n",
    "    \"PRETRAIN_ESM_CHECKPOINT_NAME\": \n",
    "        #\"facebook/esm2_t48_15B_UR50D\", \n",
    "        \"facebook/esm2_t36_3B_UR50D\",\n",
    "        # #\"facebook/esm2_t33_650M_UR50D\", \n",
    "        # #\"facebook/esm2_t12_35M_UR50D\", \n",
    "        # #\"facebook/esm2_t6_8M_UR50D\", \n",
    "\n",
    "    \"UNIPROT_PATH\": \"/home/gdallagl/myworkdir/ESMSec/data/UniProt/human_proteome.tsv\"\n",
    "}\n",
    "config[\"PRECOMPUTED_EMBS_PATH\"] = os.path.join(\n",
    "    f\"/home/gdallagl/myworkdir/ESMSec/data/UniProt/precomputed_embs\",\n",
    "    f\"entire_reviewed_uniprot_{config['PRETRAIN_ESM_CHECKPOINT_NAME'].replace('/', '-')}_\"\n",
    "    f\"{config['TYPE_EMB_FOR_CLASSIFICATION']}.safetensors\"\n",
    ")\n",
    "print(config[\"PRECOMPUTED_EMBS_PATH\"])\n",
    "config[\"PRECOMPUTED_EMBS_PATH_PROTEIN_NAMES\"] = config[\"PRECOMPUTED_EMBS_PATH\"].replace('.safetensors', '_names.json')\n",
    "config[\"PRECOMPUTED_EMBS_PATH_ADATA\"] = config[\"PRECOMPUTED_EMBS_PATH\"].replace('.safetensors', '.h5ad')\n",
    "\n",
    "\n",
    "# Initializations\n",
    "random.seed(config[\"SEED\"])\n",
    "np.random.seed(config[\"SEED\"])\n",
    "torch.manual_seed(config[\"SEED\"])\n",
    "torch.backends.cudnn.benchmark = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aefa7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899dd25",
   "metadata": {},
   "source": [
    "## Instantiate ESM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ESM model\n",
    "esm_model = AutoModel.from_pretrained(\n",
    "    config[\"PRETRAIN_ESM_CHECKPOINT_NAME\"], \n",
    "    cache_dir=config[\"PRETRAIN_ESM_CACHE_DIR\"],\n",
    "    weights_only=False  \n",
    ").to(config[\"DEVICE\"]) \n",
    "\n",
    "# Checj whcih model has been moded by AutoModel.from_pretrained()\n",
    "print(\"\\nESM model type\", type(esm_model), \"\\n\")\n",
    "\n",
    "# Load relative tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"PRETRAIN_ESM_CHECKPOINT_NAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b5151",
   "metadata": {},
   "source": [
    "## Load dataset and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREDY CHECJ PROTEION S(HUMAN, REVIWED, ...)\n",
    "uniprot_proteome = pd.read_csv(config[\"UNIPROT_PATH\"])\n",
    "\n",
    "#uniprot_proteome = uniprot_proteome.head(2)\n",
    "\n",
    "# Remae\n",
    "uniprot_proteome = uniprot_proteome.rename(columns={\"Entry\": \"protein\", \"Sequence\": \"sequence\", \"Gene Names (primary)\": \"gene\"})\n",
    "\n",
    "# For Later\n",
    "num_samples = uniprot_proteome.shape[0]\n",
    "emb_dim = esm_model.config.hidden_size\n",
    "print(num_samples, emb_dim)\n",
    "\n",
    "# ATTENTION:trucnat seqq to enforce max conetxt of EMS\n",
    "uniprot_proteome[\"truncated_sequence\"] = uniprot_proteome.sequence.apply(my_dataset.truncate_sequence)\n",
    "\n",
    "# tokenize truncated seqeunces\n",
    "    # ATTENTION: they a\n",
    "print(\"Tokenizing...\")\n",
    "encoded = tokenizer(\n",
    "    list(uniprot_proteome.truncated_sequence),\n",
    "    padding='max_length',\n",
    "    max_length=config[\"PROTEIN_MAX_LENGTH\"],\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "# add to dict\n",
    "input_ids_tensor = encoded[\"input_ids\"]          # shape: (N, L)\n",
    "attention_mask_tensor = encoded[\"attention_mask\"]\n",
    "\n",
    "#####################\n",
    "\n",
    "# save all information needed to tothe model\n",
    "cache_data = {\n",
    "    'protein': list(uniprot_proteome.protein),\n",
    "    'sequence': list(uniprot_proteome.sequence),\n",
    "    'truncated_sequence': list(uniprot_proteome.truncated_sequence),\n",
    "    'input_ids': input_ids_tensor,\n",
    "    'attention_mask': attention_mask_tensor,\n",
    "    'embedding': torch.zeros((num_samples, 1), dtype=torch.float32), # FAKE\n",
    "    'label': torch.zeros((num_samples, 1), dtype=torch.float32), # FAKE\n",
    "    'set': torch.zeros((num_samples, 1), dtype=torch.float32), # FAKE\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57455197",
   "metadata": {},
   "source": [
    "## Create embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acd709",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset.create_uniprot_embs(\n",
    "    esm_model,\n",
    "    config, \n",
    "    cache_data, \n",
    "    num_samples, \n",
    "    emb_dim,\n",
    "    save_embs_path=config[\"PRECOMPUTED_EMBS_PATH\"],\n",
    "    save_names_path=config[\"PRECOMPUTED_EMBS_PATH_PROTEIN_NAMES\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41843a",
   "metadata": {},
   "source": [
    "# Create AnnData with protein embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etnqir0n68",
   "metadata": {},
   "source": [
    "## How to Load Embeddings in Downstream Scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24637ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs, protein_names_selected = my_dataset.load_embs_safetensor(\n",
    "    precomputed_embs_path=config[\"PRECOMPUTED_EMBS_PATH\"], # path with tensors\n",
    "    protein_names_path=config[\"PRECOMPUTED_EMBS_PATH\"].replace('.safetensors', '_names.json'), # path with the prtein names of the tenoser\n",
    "    protein_to_select=None # take all\n",
    "    )\n",
    "embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779c938",
   "metadata": {},
   "source": [
    "## Create Protein anndata\n",
    "\n",
    "X --> toekn IDs of truncated protein\n",
    "\n",
    "obsm[X_latent]\n",
    "\n",
    "obsm[X_pca]\n",
    "\n",
    "uns[vocabulary] --> Id to aa mapping\n",
    "\n",
    "obs: copy from uniprot_proteim.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read matrix\n",
    "# adata = sc.AnnData(embs.cpu().numpy()) # Transpose to get Prot x dims\n",
    "\n",
    "# # Assign protein anmes names\n",
    "# adata.obs_names = protein_names_selected\n",
    "# #adata.var_names = genes.gene.astype(str)\n",
    "\n",
    "# adata.obsm[\"input_ids\"] = input_ids_tensor.cpu().numpy()\n",
    "# adata.obsm[\"attention_mask\"] = attention_mask_tensor.cpu().numpy()\n",
    "\n",
    "# adata.obs = adata.obs.merge(uniprot_proteome, left_index=True, right_on=\"protein\", how=\"left\")\n",
    "# adata.uns[\"vocabulary\"] = tokenizer.get_vocab()\n",
    "\n",
    "\n",
    "# print(adata)\n",
    "# display(adata.obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab22fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read matrix\n",
    "adata = sc.AnnData(input_ids_tensor.cpu().numpy()) # Transpose to get Prot x dims\n",
    "\n",
    "# Assign protein anmes names\n",
    "adata.obs_names = protein_names_selected\n",
    "#adata.var_names = genes.gene.astype(str)\n",
    "\n",
    "adata.obsm[\"X_all\"] = embs.cpu().numpy()\n",
    "adata.layers[\"attention_mask\"] = attention_mask_tensor.cpu().numpy()\n",
    "\n",
    "adata.obs = adata.obs.merge(uniprot_proteome, left_index=True, right_on=\"protein\", how=\"left\", validate=\"1:1\")\n",
    "adata.obs.set_index(\"protein\", drop=False)\n",
    "adata.uns[\"vocabulary\"] = { #  problem with serialization\n",
    "    str(k).replace('-', 'DASH').replace('.', 'DOT'): str(v) \n",
    "    for k, v in tokenizer.get_vocab().items()\n",
    "}\n",
    "\n",
    "print(adata)\n",
    "display(adata.obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88773d55",
   "metadata": {},
   "source": [
    "## Compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb2330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARAMETERS ---\n",
    "N_PCs = 200                # max number of components to compute\n",
    "threshold = 0.5            # desired cumulative variance explained (e.g. 50%)\n",
    "\n",
    "# --- RUN PCA ---\n",
    "X = adata.obsm[\"X_all\"]  # attention use ESM latntespace\n",
    "pca = PCA(n_components=N_PCs, random_state=42, svd_solver=\"arpack\")\n",
    "adata.obsm[\"X_pca\"] = pca.fit_transform(X)\n",
    "\n",
    "# --- STORE PCA METADATA (like Scanpy does) ---\n",
    "adata.uns[\"pca\"] = {}\n",
    "adata.uns[\"pca\"][\"variance\"] = pca.explained_variance_.copy()\n",
    "adata.uns[\"pca\"][\"variance_ratio\"] = pca.explained_variance_ratio_.copy()\n",
    "\n",
    "# --- COMPUTE CUMULATIVE VARIANCE ---\n",
    "var_ratio = pca.explained_variance_ratio_\n",
    "cumulative = np.cumsum(var_ratio)\n",
    "\n",
    "# --- FIND MINIMUM NUMBER OF PCs REACHING THRESHOLD ---\n",
    "n_pcs_optimal = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "# --- SUBSET PCA EMBEDDINGS ---\n",
    "print(\"Before:\", adata.obsm[\"X_pca\"].shape)\n",
    "adata.obsm[\"X_pca\"] = adata.obsm[\"X_pca\"][:, :n_pcs_optimal]\n",
    "print(\"After:\", adata.obsm[\"X_pca\"].shape)\n",
    "\n",
    "print(f\"To explain {threshold*100:.0f}% of variance, keep {n_pcs_optimal} PCs.\")\n",
    "\n",
    "# --- PLOT CUMULATIVE VARIANCE ---\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumulative, marker='o', lw=1)\n",
    "plt.axhline(y=threshold, color='r', ls='--', label=f'{threshold*100:.0f}% variance')\n",
    "plt.axvline(x=n_pcs_optimal, color='g', ls='--', label=f'{n_pcs_optimal} PCs')\n",
    "plt.xlabel('Number of PCs')\n",
    "plt.ylabel('Cumulative variance explained')\n",
    "plt.title('Cumulative PCA variance explained')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2f26a",
   "metadata": {},
   "source": [
    "## Compute Umaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neighbors ---\n",
    "print(\"Calculating Neightbors...\")\n",
    "sc.pp.neighbors(adata, use_rep=\"X_pca\", n_neighbors=10, n_pcs=n_pcs_optimal, key_added=\"neighbors_pca\") # overwirtes previosn grpah\n",
    "sc.pp.neighbors(adata, use_rep=\"X_all\", n_neighbors=10, key_added=\"neighbors_all\")# Use raw adata.X\n",
    "\n",
    "# --- UMAPs ---\n",
    "print(\"Calculating Umaps...\")\n",
    "sc.tl.umap(adata, random_state=42, neighbors_key=\"neighbors_pca\", key_added=\"X_umap_pca\") # each time overweites X_umap\n",
    "sc.tl.umap(adata, random_state=42, neighbors_key=\"neighbors_all\", key_added=\"X_umap_all\") # each time overweites X_umap\n",
    "\n",
    "# ATTENTION:  if those 100 PCs already capture >99% of variance, the neighbor graphs will be nearly identical\n",
    "\n",
    "\n",
    "fig, axes = my_embs.plot_embeddings(\n",
    "    adata,\n",
    "    basis=[\"X_umap_pca\", \"X_umap_all\"],\n",
    "    color=[None],\n",
    "    title=[\"UMAP (neighbors from pca space)\", \"UMAP (neighbors from all features)\"],\n",
    "    size=[5],\n",
    "    #palette=[None, \"tab20\", \"viridis\"],\n",
    "    ncols=2,  # 2 columns, 2 rows\n",
    "    figsize=(16, 8), \n",
    "    legend_loc=['on data']\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "adata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60f6d4",
   "metadata": {},
   "source": [
    "## Save Adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad(config[\"PRECOMPUTED_EMBS_PATH_ADATA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b337da",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
