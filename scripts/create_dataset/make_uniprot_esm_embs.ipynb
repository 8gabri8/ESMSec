{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb7dcbc",
   "metadata": {},
   "source": [
    "# Make ESM embs of entire reviewed Unirpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "643d1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Working directory successfully changed to: /home/gdallagl/myworkdir/ESMSec\n",
      "2.5.0+cu121\n"
     ]
    }
   ],
   "source": [
    "# Enable autoreload extension\n",
    "%load_ext autoreload\n",
    "# Automatically reload all modules (except built-ins) before executing code\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.chdir(\"/home/gdallagl/myworkdir/ESMSec\")\n",
    "print(f\"Working directory successfully changed to: {os.getcwd()}\")\n",
    "\n",
    "import utils.my_functions as mf\n",
    "import utils.models as my_models\n",
    "import utils.dataset as my_dataset\n",
    "import utils.embeddings_functions as my_embs\n",
    "import utils.scanning as my_scanning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bce7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gdallagl/myworkdir/ESMSec/data/cell_cycle/precomputed_embs/entire_reviewed_uniprot_facebook-esm2_t6_8M_UR50D_cls.pt\n"
     ]
    }
   ],
   "source": [
    "# Configuration / hyperparameters\n",
    "config = {\n",
    "    \"SEED\": 42,             # Random seed\n",
    "\n",
    "    \"PROTEIN_MAX_LENGTH\": 1000, # Max protein length (for ESM2)\n",
    "    \"PRETRAIN_ESM_CHECKPOINT_NAME\": \"facebook/esm2_t33_650M_UR50D\",#\"facebook/esm2_t33_650M_UR50D\", #\"facebook/esm2_t12_35M_UR50D\", #\"facebook/esm2_t6_8M_UR50D\", # ESM2 model name --> if nto isntalled, autocally dowlaoded\n",
    "    \"PRETRAIN_ESM_CACHE_DIR\": \"/home/gdallagl/myworkdir/data/esm2-models\", # ESM2 model cache dir\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\", # Device to use (cuda or cpu)\n",
    "    \"BATCH_SIZE\": 128,\n",
    "\n",
    "    \"UNIPROT_PATH\": \"/home/gdallagl/myworkdir/ESMSec/data/secreted/dataset_secreted.csv\", #'/home/gdallagl/myworkdir/ESMSec/data/cell_cycle/datasets/cc_dataset_final.csv', #\"/home/gdallagl/myworkdir/ESMSec/data/cell_cycle/datasets/nucleolus_final_dataset.csv\", #\"/home/gdallagl/myworkdir/ESMSec/data/cell_cycle/cell-cycle-dataset_2:3.csv\",  #\"/home/gdallagl/myworkdir/ESMSec/data/cell_cycle/only-guaranteed_cell-cycle-dataset_2:3.csv\", #\"/home/gdallagl/myworkdir/ESMSec/data/cell_cycle/cell-cycle-dataset_2:3.csv\", \n",
    "    \"TYPE_EMB_FOR_CLASSIFICATION\": \"cls\", #\"concat(agg_mean, agg_max)\", #\"contextualized_embs\", #\"concat(agg_mean, agg_max)\",cls, stacked_linearized_all\n",
    "\n",
    "    \"UNIPROT_PATH\": \"/home/gdallagl/myworkdir/ESMSec/data/UniProt/human_proteome.tsv\"\n",
    "}\n",
    "config[\"PRECOMPUTED_EMBS_PATH\"] = os.path.join(\n",
    "    f\"/home/gdallagl/myworkdir/ESMSec/data/cell_cycle/precomputed_embs\",\n",
    "    f\"entire_reviewed_uniprot_{config['PRETRAIN_ESM_CHECKPOINT_NAME'].replace('/', '-')}_\"\n",
    "    f\"{config['TYPE_EMB_FOR_CLASSIFICATION']}.pt\"\n",
    ")\n",
    "print(config[\"PRECOMPUTED_EMBS_PATH\"])\n",
    "\n",
    "# Initializations\n",
    "random.seed(config[\"SEED\"])\n",
    "np.random.seed(config[\"SEED\"])\n",
    "torch.manual_seed(config[\"SEED\"])\n",
    "torch.backends.cudnn.benchmark = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899dd25",
   "metadata": {},
   "source": [
    "# Instantiate ESM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0ed8538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ESM model type <class 'transformers.models.esm.modeling_esm.EsmModel'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ESM model\n",
    "esm_model = AutoModel.from_pretrained(config[\"PRETRAIN_ESM_CHECKPOINT_NAME\"],  cache_dir=config[\"PRETRAIN_ESM_CACHE_DIR\"]).to(config[\"DEVICE\"])\n",
    "# Checj whcih model has been moded by AutoModel.from_pretrained()\n",
    "print(\"\\nESM model type\", type(esm_model), \"\\n\")\n",
    "\n",
    "# Load relative tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"PRETRAIN_ESM_CHECKPOINT_NAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b5151",
   "metadata": {},
   "source": [
    "# Load dataset and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c9a181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(config[\"UNIPROT_PATH\"], sep=\"\\t\")\n",
    "\n",
    "# only reviewd\n",
    "data = data[data.Reviewed == \"reviewed\"]\n",
    "\n",
    "# Remae\n",
    "data = data[[\"Entry\", \"Sequence\"]].rename(columns={\"Entry\": \"protein\", \"Sequence\": \"sequence\"})\n",
    "\n",
    "# For Later\n",
    "num_samples = data.shape[0]\n",
    "\n",
    "# ATTENTION:trucnat seqq to enforce max conetxt of EMS\n",
    "data[\"truncated_sequence\"] = data.sequence.apply(my_dataset.truncate_sequence)\n",
    "\n",
    "# tokenize truncated seqeunces\n",
    "    # ATTENTION: they a\n",
    "print(\"Tokenizing...\")\n",
    "encoded = tokenizer(\n",
    "    list(data.truncated_sequence),\n",
    "    padding='max_length',\n",
    "    max_length=config[\"PROTEIN_MAX_LENGTH\"],\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "# add to dict\n",
    "input_ids_tensor = encoded[\"input_ids\"]          # shape: (N, L)\n",
    "attention_mask_tensor = encoded[\"attention_mask\"]\n",
    "# add to df\n",
    "data[\"input_ids\"] = [tensor for tensor in encoded[\"input_ids\"]]\n",
    "data[\"attention_mask\"] = [tensor for tensor in encoded[\"attention_mask\"]]\n",
    "\n",
    "#####################\n",
    "\n",
    "# save all information needed to tothe model\n",
    "cache_data = {\n",
    "    'protein': list(data.protein),\n",
    "    'sequence': list(data.sequence),\n",
    "    'truncated_sequence': list(data.truncated_sequence),\n",
    "    'input_ids': input_ids_tensor,\n",
    "    'attention_mask': attention_mask_tensor,\n",
    "    'embedding': torch.zeros((num_samples, 1), dtype=torch.float32), # FAKE\n",
    "    'label': torch.zeros((num_samples, 1), dtype=torch.float32), # FAKE\n",
    "    'set': torch.zeros((num_samples, 1), dtype=torch.float32), # FAKE\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57455197",
   "metadata": {},
   "source": [
    "# Create embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a33ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing protein batches:   0%|          | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing protein batches:   9%|▉         | 14/160 [00:26<04:37,  1.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m         \u001b[38;5;66;03m# Store batch results\u001b[39;00m\n\u001b[32m     34\u001b[39m         protein_names.extend(batch_proteins)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         embeddings_list.append(\u001b[43mbatch_embeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaving embeddings to fast PyTorch cache...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m torch.save(\n\u001b[32m     39\u001b[39m     {\n\u001b[32m     40\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprotein\u001b[39m\u001b[33m\"\u001b[39m: protein_names,\n\u001b[32m     41\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m: torch.cat(embeddings_list, dim=\u001b[32m0\u001b[39m), \u001b[38;5;66;03m#[e.cpu() for e in embeddings_list]  # list of tensors\u001b[39;00m\n\u001b[32m     42\u001b[39m     }, \n\u001b[32m     43\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mPRECOMPUTED_EMBS_PATH\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "esm_model.eval()\n",
    "\n",
    "#Cr eate dataloader for batched processing\n",
    "dataloader = my_dataset.create_dataloader(\n",
    "    cache_data,\n",
    "    batch_size=config.get(\"BATCH_SIZE\", 32),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "embeddings_list = []\n",
    "protein_names = []\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Processing protein batches\"):\n",
    "\n",
    "    batch_input_ids = batch[\"input_ids\"].to(config[\"DEVICE\"])\n",
    "    batch_attention_mask = batch[\"attention_mask\"].to(config[\"DEVICE\"])\n",
    "    batch_proteins = batch[\"name\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_esm = esm_model(\n",
    "            input_ids=batch_input_ids, \n",
    "            attention_mask=batch_attention_mask, \n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        # Get spefici embs from geena contextualised embs\n",
    "        batch_embeddings = my_models.get_embs_from_context_embs( \n",
    "                                                    context_embs_esm = outputs_esm.last_hidden_state,\n",
    "                                                    attention_mask = batch_attention_mask,\n",
    "                                                    type_embs = config[\"TYPE_EMB_FOR_CLASSIFICATION\"],\n",
    "                                                    exclude_cls=True)\n",
    "\n",
    "        # Store batch results\n",
    "        protein_names.extend(batch_proteins)\n",
    "        embeddings_list.append(batch_embeddings.detach().cpu())\n",
    "\n",
    "print(\"Saving embeddings to fast PyTorch cache...\")\n",
    "torch.save(\n",
    "    {\n",
    "        \"protein\": protein_names,\n",
    "        \"embedding\": torch.cat(embeddings_list, dim=0), #[e.cpu() for e in embeddings_list]  # list of tensors\n",
    "    }, \n",
    "    config[\"PRECOMPUTED_EMBS_PATH\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etnqir0n68",
   "metadata": {},
   "source": [
    "# How to Load Embeddings in Downstream Scripts\n",
    "\n",
    "Save embs in Torch format. \n",
    "\n",
    "Save a csv with Protein | index in array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87be9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading back from file: /home/gdallagl/myworkdir/ESMSec/data/cell_cycle/precomputed_embs/entire_reviewed_uniprot_facebook-esm2_t6_8M_UR50D_cls.pt...\n",
      "✓ Loaded 1000 embeddings (dim: 320)\n",
      "✓ Loaded 1000 embeddings for your dataset\n",
      "  Shape: torch.Size([1000, 320])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reading back from file: {config['PRECOMPUTED_EMBS_PATH']}...\")\n",
    "\n",
    "# Load the precomputed embeddings\n",
    "emb_dict_precomputed = torch.load(config[\"PRECOMPUTED_EMBS_PATH\"], weights_only=False)\n",
    "all_proteins = emb_dict_precomputed[\"protein\"]  # List of protein names\n",
    "all_embeddings = emb_dict_precomputed[\"embedding\"]  # Tensor: (N_all, emb_dim)\n",
    "\n",
    "print(f\"✓ Loaded {len(all_proteins)} embeddings (dim: {all_embeddings.shape[1]})\")\n",
    "\n",
    "# Create a mapping for fast lookup: protein_name -> index\n",
    "protein_to_idx = {protein: idx for idx, protein in enumerate(all_proteins)}\n",
    "\n",
    "# Extract embeddings for proteins in cache_data in the correct order\n",
    "subset_embeddings = []\n",
    "missing_proteins = []\n",
    "\n",
    "for p in cache_data[\"protein\"]:\n",
    "    if p in protein_to_idx:\n",
    "        idx = protein_to_idx[p]\n",
    "        subset_embeddings.append(all_embeddings[idx])\n",
    "    else:\n",
    "        missing_proteins.append(p)\n",
    "        print(f\"⚠ Protein {p} not found in precomputed embeddings.\")\n",
    "\n",
    "# Stack into a single tensor\n",
    "if subset_embeddings:\n",
    "    subset_embeddings_tensor = torch.stack(subset_embeddings)  # Shape: (N_subset, emb_dim)\n",
    "\n",
    "    # Update cache_data with the embeddings\n",
    "    cache_data[\"embedding\"] = subset_embeddings_tensor\n",
    "\n",
    "    print(f\"✓ Loaded {len(subset_embeddings)} embeddings for your dataset\")\n",
    "    print(f\"  Shape: {subset_embeddings_tensor.shape}\")\n",
    "\n",
    "if missing_proteins:\n",
    "    print(f\"⚠ Total missing: {len(missing_proteins)} proteins\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
